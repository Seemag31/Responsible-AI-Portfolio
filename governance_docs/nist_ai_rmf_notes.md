# NIST AI Risk Management Framework (AI RMF) — Notes

## 1. Purpose of the Framework
The NIST AI RMF provides a voluntary structure to help organizations identify, assess, manage, and monitor risks associated with AI systems. It promotes trustworthy AI by embedding governance, documentation, and oversight throughout the system lifecycle rather than treating risk as a one-time technical issue.

## 2. Definition of AI Risk
AI risk refers to the potential for AI systems to cause harm to individuals, groups, organizations, or society. Risk depends on both the likelihood of harm occurring and the severity of its impact. Harms may include bias, discrimination, safety failures, privacy breaches, security vulnerabilities, and erosion of public trust.

## 3. Core Functions

### Govern
Focuses on establishing organizational policies, culture, accountability structures, and oversight mechanisms for AI systems.  
Example actions: defining AI governance roles, setting risk tolerance, maintaining documentation practices.

### Map
Involves understanding the AI system’s context, intended use, stakeholders, and potential impacts.  
Example actions: identifying affected populations, defining system boundaries, documenting use scenarios.

### Measure
Centers on evaluating AI systems for performance, fairness, reliability, robustness, and safety.  
Example actions: bias testing, accuracy evaluation, robustness checks.

### Manage
Addresses prioritizing, mitigating, and responding to identified risks.  
Example actions: adjusting system design, restricting high-risk uses, adding human oversight.

### Monitor
Emphasizes ongoing tracking of AI system behavior after deployment.  
Example actions: performance monitoring, incident reporting, drift detection.

## 4. Lifecycle Coverage
The RMF applies across the entire AI lifecycle, from design and development to deployment and post-deployment monitoring. Risk management is presented as continuous and iterative rather than a one-time checklist.

## 5. Stakeholders & Roles
AI risk management responsibilities are distributed across leadership (policy and oversight), technical teams (implementation and testing), risk and compliance teams (review and accountability), and external stakeholders (users and affected communities).

## 6. Practical Takeaways
AI projects should begin with a documented purpose and risk context. Fairness and safety testing should be systematic rather than ad hoc. Monitoring plans should be defined before deployment, and documentation should be treated as a core part of risk management.

## 7. Limitations or Gaps
The framework is non-prescriptive and does not specify exact technical metrics. Adoption is voluntary, and effective implementation requires coordination and resources across multiple organizational roles.

## 8. Key Quote or Concept to Remember
> AI risk management is an ongoing lifecycle process, not a one-time compliance task.

